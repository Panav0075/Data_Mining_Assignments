# -*- coding: utf-8 -*-
"""HW2_Data Mining

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T09oPr5XH8gZuEyZ0j7t541EeDxLl40m

<h1>TASK 1: TITANIC CHALLENGE <h1>
"""

# Shortened and robust Titanic analysis
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

print("="*70)
print("TASK 1: TITANIC CHALLENGE")
print("="*70)

# Load data
try:
    train_df = pd.read_csv('train.csv')
except:
    # If file not found, try alternate path
    train_df = pd.read_csv('/content/train.csv')

print(f"\nOriginal shape: {train_df.shape}")
print(f"Columns: {list(train_df.columns)}")
print(f"\nMissing values:\n{train_df.isnull().sum()[train_df.isnull().sum() > 0]}")

# ============================================================================
# SUBTASK 1: DATA PREPROCESSING
# ============================================================================

print(f"\n{'='*70}")
print("SUBTASK 1: DATA PREPROCESSING")
print(f"{'='*70}")

def preprocess_titanic(df):
    """
    Compact preprocessing pipeline for Titanic dataset

    Args:
        df: Raw Titanic dataframe

    Returns:
        X: Feature matrix
        y: Target labels (Survived)
    """
    data = df.copy()

    # STEP 1: Handle missing values
    data['Age'].fillna(data.groupby(['Pclass', 'Sex'])['Age'].transform('median'), inplace=True)
    data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)
    data['Fare'].fillna(data['Fare'].median(), inplace=True)

    # STEP 2: Feature engineering
    data['Family_Size'] = data['SibSp'] + data['Parch'] + 1
    data['Is_Alone'] = (data['Family_Size'] == 1).astype(int)
    data['Has_Cabin'] = data['Cabin'].notna().astype(int)

    # STEP 3: Extract and encode titles from names
    data['Title'] = data['Name'].str.extract(r' ([A-Za-z]+)\.', expand=False)
    title_map = {'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Rare': 4}
    data['Title'] = data['Title'].map(lambda x: title_map.get(x, 4))

    # STEP 4: Encode categorical variables
    data['Sex'] = LabelEncoder().fit_transform(data['Sex'])
    data['Embarked'] = LabelEncoder().fit_transform(data['Embarked'])

    # STEP 5: Select final feature set
    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',
                'Embarked', 'Family_Size', 'Is_Alone', 'Has_Cabin', 'Title']

    return data[features], data['Survived']

X, y = preprocess_titanic(train_df)
print(f"\nPreprocessing complete:")
print(f"  Features: {X.shape[1]}")
print(f"  Samples: {X.shape[0]}")
print(f"  Feature names: {list(X.columns)}")
print(f"  Survival rate: {y.mean():.1%}")
print(f"  No missing values: {X.isnull().sum().sum() == 0}")

# ============================================================================
# SUBTASK 2: DECISION TREE MODEL
# ============================================================================

print(f"\n{'='*70}")
print("SUBTASK 2: DECISION TREE MODEL")
print(f"{'='*70}")

# Initialize Decision Tree with fine-tuned parameters
dt_model = DecisionTreeClassifier(criterion='gini',max_depth=5,min_samples_split=4,min_samples_leaf=2,random_state=42)
dt_model.fit(X, y)

print("\nDecision Tree Parameters:")
for param, value in dt_model.get_params().items():
    if param in ['criterion', 'max_depth', 'min_samples_split', 'min_samples_leaf']:
        print(f"  {param}: {value}")

dt_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': dt_model.feature_importances_
}).sort_values('Importance', ascending=False)

print("\nFeature Importance Rankings:")
for idx, row in dt_importance.iterrows():
    if row['Importance'] > 0.01:  # Show features with >1% importance
        print(f"  {row['Feature']:<15}: {row['Importance']:.4f}")

print("\nKey Observations:")
print(f"  - Most important feature: {dt_importance.iloc[0]['Feature']}")
print(f"  - Top 3 features account for {dt_importance.head(3)['Importance'].sum():.1%} of decisions")

# ============================================================================
# SUBTASK 3: FIVE-FOLD CROSS VALIDATION - DECISION TREE
# ============================================================================

print(f"\n{'='*70}")
print("SUBTASK 3: FIVE-FOLD CROSS VALIDATION - DECISION TREE")
print(f"{'='*70}")

# Perform 5-fold cross-validation
dt_cv_scores = cross_val_score(dt_model, X, y, cv=5, scoring='accuracy')

print("\nCross-Validation Results (Decision Tree):")
for fold, score in enumerate(dt_cv_scores, 1):
    print(f"  Fold {fold}: {score*100:.2f}%")

print(f"\nSummary Statistics:")
print(f"  Mean Accuracy:     {dt_cv_scores.mean()*100:.2f}%")
print(f"  Std Deviation:     {dt_cv_scores.std()*100:.2f}%")
print(f"  Min Accuracy:      {dt_cv_scores.min()*100:.2f}%")
print(f"  Max Accuracy:      {dt_cv_scores.max()*100:.2f}%")
print(f"  95% Confidence:    [{(dt_cv_scores.mean()-1.96*dt_cv_scores.std())*100:.2f}%, {(dt_cv_scores.mean()+1.96*dt_cv_scores.std())*100:.2f}%]")

print("\nInterpretation:")
print(f"  - The model achieves consistent performance across folds")
print(f"  - Standard deviation of {dt_cv_scores.std()*100:.2f}% indicates {'high' if dt_cv_scores.std() > 0.02 else 'low'} variance")

# ============================================================================
# SUBTASK 4: FIVE-FOLD CROSS VALIDATION - RANDOM FOREST
# ============================================================================

print(f"\n{'='*70}")
print("SUBTASK 4: FIVE-FOLD CROSS VALIDATION - RANDOM FOREST")
print(f"{'='*70}")

# Initialize Random Forest with optimized parameters
rf_model = RandomForestClassifier(n_estimators=100,max_depth=8,min_samples_split=4,min_samples_leaf=2,max_features='sqrt',criterion='gini',random_state=42,n_jobs=-1)
rf_model.fit(X, y)

print("\nRandom Forest Parameters:")
for param, value in rf_model.get_params().items():
    if param in ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'max_features']:
        print(f"  {param}: {value}")

# Perform 5-fold cross-validation
rf_cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='accuracy')

print("\nCross-Validation Results (Random Forest):")
for fold, score in enumerate(rf_cv_scores, 1):
    print(f"  Fold {fold}: {score*100:.2f}%")

print(f"\nSummary Statistics:")
print(f"  Mean Accuracy:     {rf_cv_scores.mean()*100:.2f}%")
print(f"  Std Deviation:     {rf_cv_scores.std()*100:.2f}%")
print(f"  Min Accuracy:      {rf_cv_scores.min()*100:.2f}%")
print(f"  Max Accuracy:      {rf_cv_scores.max()*100:.2f}%")
print(f"  95% Confidence:    [{(rf_cv_scores.mean()-1.96*rf_cv_scores.std())*100:.2f}%, {(rf_cv_scores.mean()+1.96*rf_cv_scores.std())*100:.2f}%]")

# Feature importance from Random Forest
rf_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

print("\nFeature Importance Rankings (Random Forest):")
for idx, row in rf_importance.iterrows():
    if row['Importance'] > 0.01:
        print(f"  {row['Feature']:<15}: {row['Importance']:.4f}")

# ============================================================================
# VISUALIZATION
# ============================================================================
print(f"\n{'='*70}")
print("GENERATING VISUALIZATIONS")
print(f"{'='*70}")

fig, axes = plt.subplots(2, 2, figsize=(20, 16))

# Plot 1: Decision Tree structure (limited depth for visibility)
plot_tree(dt_model, feature_names=X.columns,
          class_names=['Died', 'Survived'], filled=True,
          ax=axes[0, 0], max_depth=3, fontsize=9)
axes[0, 0].set_title('Decision Tree Structure (max_depth=3 shown for clarity)',
                      fontsize=14, fontweight='bold')

# Plot 2: Feature importance comparison
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Decision Tree': dt_model.feature_importances_,
    'Random Forest': rf_model.feature_importances_
}).sort_values('Random Forest', ascending=True)

importance_df.plot(x='Feature', kind='barh', ax=axes[0, 1],
                   color=['skyblue', 'coral'], alpha=0.8)
axes[0, 1].set_title('Feature Importance Comparison', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('Importance Score', fontsize=12)
axes[0, 1].legend(['Decision Tree', 'Random Forest'])
axes[0, 1].grid(alpha=0.3, axis='x')

# Plot 3: Cross-validation score distribution
bp = axes[1, 0].boxplot([dt_cv_scores*100, rf_cv_scores*100],
                         labels=['Decision Tree', 'Random Forest'],
                         patch_artist=True, widths=0.6)
bp['boxes'][0].set_facecolor('skyblue')
bp['boxes'][1].set_facecolor('coral')
axes[1, 0].set_ylabel('Accuracy (%)', fontsize=12)
axes[1, 0].set_title('Cross-Validation Accuracy Distribution', fontsize=14, fontweight='bold')
axes[1, 0].grid(alpha=0.3, axis='y')
axes[1, 0].set_ylim([75, 90])

# Plot 4: Mean accuracy with error bars
means = [dt_cv_scores.mean()*100, rf_cv_scores.mean()*100]
stds = [dt_cv_scores.std()*100, rf_cv_scores.std()*100]
x_pos = np.arange(len(['Decision Tree', 'Random Forest']))
axes[1, 1].bar(x_pos, means, yerr=stds,
               capsize=10, color=['skyblue', 'coral'], alpha=0.8, width=0.6)
axes[1, 1].set_xticks(x_pos)
axes[1, 1].set_xticklabels(['Decision Tree', 'Random Forest'])
axes[1, 1].set_ylabel('Accuracy (%)', fontsize=12)
axes[1, 1].set_title('Mean Accuracy ± Standard Deviation', fontsize=14, fontweight='bold')
axes[1, 1].grid(alpha=0.3, axis='y')
axes[1, 1].set_ylim([75, 90])

# Add accuracy values on bars
for i, (mean, std) in enumerate(zip(means, stds)):
    axes[1, 1].text(i, mean + std + 0.5, f'{mean:.2f}%',
                    ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.savefig('titanic_analysis.png', dpi=300, bbox_inches='tight')
print("\n✓ Visualization saved as 'titanic_analysis.png'")
plt.show()

# ============================================================================
# SUBTASK 5: COMPARISON AND ANALYSIS
# ============================================================================

print(f"\n{'='*70}")
print("SUBTASK 5: ALGORITHM COMPARISON AND ANALYSIS")
print(f"{'='*70}")

# Statistical comparison
t_stat, p_val = stats.ttest_rel(rf_cv_scores, dt_cv_scores)
accuracy_improvement = (rf_cv_scores.mean() - dt_cv_scores.mean()) * 100
variance_reduction = (1 - rf_cv_scores.std() / dt_cv_scores.std()) * 100

# Performance summary table
print(f"\nPerformance Comparison:")
print(f"┌{'─'*40}┬{'─'*12}┬{'─'*12}┬{'─'*12}┐")
print(f"│ {'Model':<38} │ {'Mean Acc':<10} │ {'Std Dev':<10} │ {'Range':<10} │")
print(f"├{'─'*40}┼{'─'*12}┼{'─'*12}┼{'─'*12}┤")
print(f"│ {'Decision Tree':<38} │ {dt_cv_scores.mean()*100:>9.2f}% │ {dt_cv_scores.std()*100:>9.2f}% │ {(dt_cv_scores.max()-dt_cv_scores.min())*100:>9.2f}% │")
print(f"│ {'Random Forest':<38} │ {rf_cv_scores.mean()*100:>9.2f}% │ {rf_cv_scores.std()*100:>9.2f}% │ {(rf_cv_scores.max()-rf_cv_scores.min())*100:>9.2f}% │")
print(f"└{'─'*40}┴{'─'*12}┴{'─'*12}┴{'─'*12}┘")

print(f"\nStatistical Analysis:")
print(f"  • Accuracy Improvement: {accuracy_improvement:+.2f}%")
print(f"  • Variance Reduction: {variance_reduction:.1f}%")
print(f"  • Paired t-test p-value: {p_val:.4f} {'✓ (statistically significant)' if p_val < 0.05 else '✗ (not significant)'}")

print(f"\n{'─'*70}")
print("DETAILED ANALYSIS AND CONCLUSIONS:")
print(f"{'─'*70}")

print("\n1. WHICH ALGORITHM IS BETTER?")
print(f"   Random Forest is superior, achieving {accuracy_improvement:.2f}% higher accuracy")
print(f"   ({rf_cv_scores.mean()*100:.2f}% vs {dt_cv_scores.mean()*100:.2f}%).")

print("\n2. KEY OBSERVATIONS:")

print("\n   a) Variance Reduction:")
print(f"      Random Forest shows {variance_reduction:.1f}% lower variance than Decision Tree")
print(f"      (σ={rf_cv_scores.std()*100:.2f}% vs σ={dt_cv_scores.std()*100:.2f}%).")
print("      This indicates more stable predictions across different data samples,")
print("      a critical property for reliable model deployment in production.")

print("\n   b) Overfitting Mitigation:")
print("      Single decision trees are highly prone to overfitting—they memorize")
print("      training data patterns including noise. Random Forest combats this through:")
print("      • Bootstrap Aggregation: Each tree trains on a different random subset")
print("        (with replacement) of the training data, exposing trees to varied patterns")
print("      • Feature Randomness: At each node split, only √11 ≈ 3 features are considered")
print("        instead of all 11, preventing dominant features from appearing in every tree")

print("\n   c) Tree Decorrelation:")
print("      The most critical advantage is decorrelation. In the Titanic dataset, 'Sex' and")
print("      'Title' are extremely strong predictors. With bagging alone, all trees would")
print("      split on these features first, creating highly correlated trees. Random Forest's")
print("      feature subsampling ensures these dominant features aren't always available,")
print("      forcing trees to explore alternative splits (Pclass, Age, Fare). This diversity")
print("      means trees make different errors, and averaging reduces overall error.")

print("\n   d) Bias-Variance Tradeoff:")
print("      Random Forest achieves an optimal bias-variance balance. Each individual tree")
print(f"      has slightly higher bias (suboptimal splits due to feature restriction), but")
print("      the ensemble dramatically reduces variance through averaging. The net effect")
print("      is lower total error, as evidenced by the improved cross-validation scores.")

